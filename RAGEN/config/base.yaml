defaults:
  - ppo_trainer # this is a symbolic link to the verl/verl/trainer/config/ppo_trainer.yaml file
  - envs

system:
  # CUDA_VISIBLE_DEVICES: "0,1,2,3" # for testing
  CUDA_VISIBLE_DEVICES: "0,1,2,3"
  RAY_EXPERIMENTAL_NOSET_CUDA_VISIBLE_DEVICES: "1"

seed:
  train: 10000
  val: 123

micro_batch_size_per_gpu: 1
ppo_mini_batch_size: 16
model_path: /scratch/gpfs/zs7353/Llama-3.1-8B-Instruct
# model_path: /scratch/gpfs/zs7353/Qwen2.5-7B-Instruct
enable_response_mask: True # Enabling response mask could improve stability of rollout/old_log_prob, as P(st|history) are no longer calculated in loss here. See https://docs.google.com/document/d/1bg7obeiKTExuHHBl5uOiSpec5uLDZ2Tgvxy6li5pHX4/edit?usp=sharing for more details.
grpo_advantage_length_weight: False
debug_mask_sanity: True # for testing

lora:
  rank: 0
  alpha: 16
  target_modules: all-linear

actor_rollout_ref:
  model:
    path: ${model_path}
    lora_rank: ${lora.rank}
    lora_alpha: ${lora.alpha}
    target_modules: ${lora.target_modules}
  actor:
    ppo_mini_batch_size: ${ppo_mini_batch_size}  # by default, ppo_mini_batch_size = train_batch_size / 4
    micro_batch_size_per_gpu: ${micro_batch_size_per_gpu} # following micro_batch_size_per_gpu
    ppo_micro_batch_size_per_gpu: ${micro_batch_size_per_gpu} # following micro_batch_size_per_gpu
    use_ref: True
    entropy_coeff: 0.001
    use_kl_loss: True 
    kl_loss_coef: 0.001
    kl_loss_type: low_var_kl
    clip_ratio_low: 0.2
    clip_ratio_high: 0.2
    grpo_advantage_length_weight: ${grpo_advantage_length_weight}
    optim:
      betas: [0.9, 0.999]
  ref:
    log_prob_micro_batch_size_per_gpu: ${micro_batch_size_per_gpu} # following micro_batch_size_per_gpu
  rollout:
    log_prob_micro_batch_size_per_gpu: ${micro_batch_size_per_gpu} # following micro_batch_size_per_gpu
    tensor_model_parallel_size: 4
    # tensor_model_parallel_size: 4 # for testing
    max_model_len: 16000
    prompt_length: 1 # useless. Just put it here
    response_length: 128 # single-turn response length
    gpu_memory_utilization: 0.35
    max_num_batched_tokens: 16000 # set only when enable_chunked_prefill is true
    temperature: 1
    rollout_filter_ratio: 0.25
    rollout_filter_type: std # max_mean or std
    enforce_eager: True #  for small models, set both enforce_eager and free_cache_engine to False to make rollout faster
    free_cache_engine: True
    val_kwargs:
      do_sample: True
      temperature: 0.5
    tp_size_check: true

critic:
  ppo_mini_batch_size: ${ppo_mini_batch_size} # by default, ppo_mini_batch_size = train_batch_size / 4
  ppo_micro_batch_size_per_gpu: ${micro_batch_size_per_gpu} # following micro_batch_size_per_gpu
  model:
    path: ${model_path}
    lora_rank: ${lora.rank}
    lora_alpha: ${lora.alpha}
    target_modules: ${lora.target_modules}
  optim:
    betas: [0.9, 0.999]

data:
  max_prompt_length: null
  max_response_length: null
  train_batch_size: null

algorithm:
  gamma: 1.0
  lam: 1.0
  high_level_gamma: 0.95
  adv_estimator: gae
  bi_level_gae: False
  use_kl_in_reward: False
  kl_penalty: low_var_kl  # how to estimate kl divergence
  kl_ctrl:
    type: adaptive
    kl_coef: 0.01
    target_kl: 0.1

trainer:
  project_name: ragen
  experiment_name: test
  total_training_steps: 200
  validation_steps: 1 # validation instances = validation_steps * val_env_groups * group_size
  val_before_train: True
  n_gpus_per_node: 4
  # n_gpus_per_node: 4 # for testing
  test_freq: 10
  generations_to_log_to_wandb: 
    train: 128 # TODO: will be implemented
    val: 20
  logger: [ 'console', 'wandb' ]

agent_proxy:
  max_context_window: -1 # set a value > 0 to enable context window for long trajectory
  max_turn: 20
  action_sep: "||"
  max_actions_per_turn: 10 # how many actions can be output at most in a single turn
  use_turn_scores: True # important to GAE when applying token-level rewards to token-level advantages. If False, will take the sum of scores as the reward for the last turn.
  enable_think: True # False -> no think RL
  reward_normalization:
    grouping: "state" # state / batch / inductive
    method: "identity" # asym_clip / identity / mean_std
  think_step_bonus:
    enabled: true
    min_tokens: 20 # threshold; must meet or exceed
    min_unique_ratio: 0.25 # at least this fraction of tokens must be unique
    only_if_base_positive: true
    scale_by: "std" # "std" | "mad" | "mean_abs"  (robust scaling choice)
    beta: 0.2 # step size as a fraction of the chosen scale

es_manager:
  format_penalty: -0.1
  train:
    env_groups: 8
    # under the same group, the env config and env seed are ensured to be equal
    group_size: 8
    env_configs:
      tags: ["lbat"]
      n_groups: [8] # If not set, all env names divide nums equally. Under the same group, the env config and env seed (prompt) are equal in each generation
  val:
    env_groups: 8
    group_size: 1 # should be set to 1 because when val temperature is set to 0 and group size > 1, there will be repetitive prompts which leads to same trajectory.
    env_configs:
      tags: ["lbat"]
      n_groups: [8] # TODO: If not set, all env names divide nums equally. Under the same group, the env config and env seed (prompt) are equal in each generation

ctx_manager:
  generation: # go to vllm
    gen_config:
      response_length: ${actor_rollout_ref.rollout.response_length}
      temperature: ${actor_rollout_ref.rollout.temperature}
      top_p: ${actor_rollout_ref.rollout.top_p}
      top_k: ${actor_rollout_ref.rollout.top_k}
      kwargs: null

# for accel
accel:
  enabled: true
  buffer_capacity: 64
  # buffer_capacity: 8 # for testing
  temperature: 0.5
  seed: 0
  # warmup_random_envs: 32
  warmup_random_envs: 4 # for testing
  generator_per_iter: 1
  n_eval_repeats: 2
  pick_policy: max_gap
  perturb_sigma_small: 0.05
  perturb_sigma_big: 0.15