# Import from general libraries
import hydra
import torch
import numpy as np
from omegaconf import OmegaConf, DictConfig
from typing import Dict, Any, Set
from tqdm import tqdm
from fastchat.model import get_conversation_template
from transformers import (
    AutoTokenizer,
    AutoModelForCausalLM,
)

# Import from our own packages
from llm_exploration.constants import (
    PARENT_DIR,
    DATAGEN_CONFIG_DIR,
)
from llm_exploration.utils.temperature_annealing_utils import get_min_p_from_temperature
from llm_exploration.utils.data_utils import write_json
from llm_exploration.bandits.llm_bandit import LLMBandit
from llm_exploration.utils.stats_utils import get_cumulative_average
from llm_exploration.bandits.bandit_utils import get_empirical_regret
from llm_exploration.utils.torch_utils import set_seed_everywhere
import llm_exploration.utils.temperature_annealing_utils as temperature_scheduler
import llm_exploration.inference as InferenceEngine
from llm_exploration.common.tokenizer_separators import get_tokenizer_separators


def evaluate_llm_on_bandit_env(
    bandit_env: LLMBandit,
    inference_engine: InferenceEngine.LLMInferenceEngine,
    randomly_set_arm_probabilities: bool,
    t_scheduler: temperature_scheduler.TemperatureScheduler,
    max_n_tokens: int,
    top_p: float,
    temperature_threshold: float,
    min_p_choice: float,
) -> Dict[str, Any]:
    """
    Runs one evaluation on the LLM Bandit environment

    Input:
        bandit_env (LLMBandit):
            The bandit environment to run evaluation on.

        inference_engine (InferenceEngine.LLMInferenceEngine):
            This is used as an API to query the desired
            language model.

        randomly_set_arm_probabilities (bool):
            Whether to keep the arms and default means of the
            associated Bernoulli distribution, or whether to
            set these values randomly.

        t_scheduler (temperature_scheduler.TemperatureScheduler):
            The scheduler to control the temperature of the
            generation at each turn.

        max_n_tokens (int):
            Number of new tokens to generate, per turn

        top_p (float):
            An alternative to sampling with temperature, called nucleus sampling,
            where the model considers the results of the tokens with top_p probability mass.
            So 0.1 means only the tokens comprising the top 10% probability mass are considered.

            See the following link for documentation:
            https://platform.openai.com/docs/api-reference/introduction

        temperature_threshold (float):
            We only use min_p sampling for temperature above this threshold

        min_p_choice (float):
            Value of p_base for min_p sampling, if used

    Output:
        A dictionary of this particular evaluation run:
        {
            "conversation": conversation,
            "all_rewards": all_rewards,
            "cumulative_average_rewards": cumulative_average_rewards,
            "empirical_regret": empirical_regret,
            "probability_list": bandit_probability_list,
            "text_actions": text_actions,
            "num_invalid_actions": num_invalid_actions,
        }

        where each thing is described below:

        conversation (List[Dict[str, str]]):
            conversation history with the language model.
            Typically has the following format.
            [
                {
                    "role": "system",
                    "content": <system_prompt>,
                },
                {
                    "role": "user",
                    "content": <user_prompt>,
                },
                {
                    "role": "assistant",
                    "content": <model_response>,
                },
                ....
            ]

        all_rewards (List[float]):
            The list of obtained rewards.
            all_rewards[i] = reward obtained at timestep i

        cumulative_average_rewards (List[float]):
            each entry is the average reward until that timestep, i.e.,
            cumulative_average_rewards[i] = \sum_{j = 0}^i all_rewards[i] / (i + 1)

        empirical_regret (List[float]):
            each entry is the empirical regret at timestep i,
            empirical_regret[i] = average reward for the best arm
                                  - cumulative_average_rewards[i]

        bandit_probability_list (List[float]):
            bandit_probability_list[i] = mean of the Bernoulli reward
                                         for arms_list[i]

        text_actions (List[str]):
            text_actions[i] = the action generated by LLM policy,
                              in text format.

        num_invalid_actions (int):
            Number of actions taken by the LLM that is invalid.
    """
    conv = get_conversation_template("gpt-4")
    if bandit_env.should_include_system_prompt_in_conversation():
        conv.set_system_message(bandit_env.get_system_prompt())

    if not randomly_set_arm_probabilities:
        probability_list = None
        arms_list = None
    else:
        probability_list = np.random.uniform(
            low=0.0,
            high=1.0,
            size=len(bandit_env.get_probability_list()),
        ).tolist()

        arms_choices = [
            "blue",
            "red",
            "green",
            "violet",
            "purple",
            "indigo",
            "teal",
            "orange",
            "pink",
            "black",
        ]
        arms_list = np.random.choice(
            arms_choices,
            size=len(bandit_env.get_probability_list()),
            replace=False,
        ).tolist()

    bandit_env.reset(
        probability_list=probability_list,
        arms_list=arms_list,
    )
    t_scheduler.reset()

    for time_step in tqdm(range(bandit_env.T)):
        user_prompt = bandit_env.get_user_prompt(time_step=time_step)
        conv.append_message(
            role="user",
            message=user_prompt,
        )

        temperature = t_scheduler.get_temperature()
        min_p = get_min_p_from_temperature(
            temperature=temperature,
            temperature_threshold=temperature_threshold,
            min_p_choice=min_p_choice,
        )

        model_response = inference_engine.generate(
            conv=conv.to_openai_api_messages(),
            max_n_tokens=max_n_tokens,
            temperature=temperature,
            top_p=top_p,
            min_p=min_p,
        )

        bandit_env.step_text_action(
            text_action=model_response,
        )

        conv.append_message(
            role="assistant",
            message=bandit_env.history[-1],
        )

        t_scheduler.step()

    all_rewards = bandit_env.get_rewards()
    cumulative_average_rewards = get_cumulative_average(
        arr=all_rewards,
    )
    empirical_regret = get_empirical_regret(
        bandit_env=bandit_env,
    )
    history = conv.to_openai_api_messages()
    bandit_probability_list = bandit_env.get_probability_list()
    text_actions = bandit_env.get_all_text_actions()
    num_invalid_actions = bandit_env.get_num_invalid_actions()

    return {
        "conversation": history,
        "all_rewards": all_rewards,
        "cumulative_average_rewards": cumulative_average_rewards,
        "empirical_regret": empirical_regret,
        "probability_list": bandit_probability_list,
        "text_actions": text_actions,
        "num_invalid_actions": num_invalid_actions,
    }


def load_inference_engine(config: Dict[str, Any]) -> InferenceEngine.LLMInferenceEngine:
    """
    Given config, loads the appropriate inference engine.

    Input:
        config (Dict):
            configuration dictionary

    Output:
        inference_engine (InferenceEngine.LLMInferenceEngine):
            the inference engine to use
    """
    model_name = config["inference_engine"]["model_name"]
    model_type = config["inference_engine"]["model_type"]
    tokenizer_name = config["inference_engine"]["tokenizer_name"]

    if model_type == "openai_api_models":
        inference_engine = InferenceEngine.OpenAIInferenceEngine(
            model_name=model_name,
        )

    elif model_type == "huggingface_models":
        model = AutoModelForCausalLM.from_pretrained(
            pretrained_model_name_or_path=model_name,
            trust_remote_code=True,
            torch_dtype=torch.bfloat16,
            attn_implementation="flash_attention_2",
        ).cuda()
        model.tie_weights()

        tokenizer = AutoTokenizer.from_pretrained(
            pretrained_model_name_or_path=tokenizer_name,
            trust_remote_code=True,
            use_fast=False,
        )

        if not config["finetuned_tokenizer"]:
            tokenizer, _ = get_tokenizer_separators(
                tokenizer=tokenizer,
                tokenizer_name=tokenizer_name,
            )
            model.resize_token_embeddings(len(tokenizer))

        inference_engine = InferenceEngine.HuggingFaceLLMInferenceEngine(
            model_name=config["inference_engine"]["base_model_name"],
            model=model,
            tokenizer=tokenizer,
        )

    else:
        raise ValueError(f"Given model_type {model_type} is not supported.")

    return inference_engine


@hydra.main(
    version_base=None,
    config_path=DATAGEN_CONFIG_DIR,
    config_name="evaluate_llm_on_bandit_config",
)
def main(config: DictConfig):
    """
    Main entry point for running evaluation on the Bandit Environment.
    Evaluates on a slightly modified version of the bandit Environment from this paper:

    Can large language models explore in-context? (https://arxiv.org/abs/2403.15371)
    """
    config.repo_dir = PARENT_DIR
    config.temperature_scheduler.kwargs.num_total_steps = config.llm_bandit.T
    OmegaConf.resolve(config)
    set_seed_everywhere(config.seed)

    missing_keys: Set[str] = OmegaConf.missing_keys(config)
    if missing_keys:
        raise ValueError(f"Got missing keys in config:\n{missing_keys}")
    config = OmegaConf.to_object(config)
    print(config)

    if (
        not torch.cuda.is_available()
        and config["inference_engine"]["model_type"] != "openai_api_models"
    ):
        raise ValueError("No GPUs available, so cannot run this script.")

    if config["finetuned_tokenizer"]:
        config["llm_bandit"]["include_special_tokens"] = True
        config["llm_bandit"]["include_text_explanation"] = True
    else:
        config["llm_bandit"]["include_special_tokens"] = False
        config["llm_bandit"]["include_text_explanation"] = False
    config["llm_bandit"]["include_system_prompt_in_conversation"] = False

    bandit_env = LLMBandit(**config["llm_bandit"])
    inference_engine = load_inference_engine(config=config)

    temperature_scheduler_class = getattr(
        temperature_scheduler,
        config["temperature_scheduler"]["temperature_scheduler_class"],
    )
    t_scheduler = temperature_scheduler_class(**config["temperature_scheduler"]["kwargs"])

    all_histories = []

    for _ in tqdm(range(config["num_trials"])):
        record = evaluate_llm_on_bandit_env(
            bandit_env=bandit_env,
            inference_engine=inference_engine,
            randomly_set_arm_probabilities=config["randomly_set_arm_probabilities"],
            t_scheduler=t_scheduler,
            max_n_tokens=config["max_new_tokens"],
            top_p=config["top_p"],
            temperature_threshold=config["temperature_threshold"],
            min_p_choice=config["min_p_choice"],
        )

        all_histories.append(record)

    print(f"Number of trials: {config['num_trials']}")
    write_json(
        data={
            "num_trials": config["num_trials"],
            "records": all_histories,
        },
        fname=config["save_file"],
    )


if __name__ == "__main__":
    main()
